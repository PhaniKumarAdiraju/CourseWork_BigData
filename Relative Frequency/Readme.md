# Calculate Relative Frequency of 100K Wiki Text 

In this assignment, I worked on a set of 100,000 Wikipedia documents: 100KWikiText.txt, in which each line consists of the plain text extracted from an individual Wikipedia document. Worked on the following using AWS instances:

1. Configured and ran the latest release of Apache Hadoop in a pseudo-distributed mode.
2. Developed a MapReduce-based approach in Hadoop system to compute the relative frequencies of each word that occurs in all the documents in 100KWikiText.txt, and output the top 100 word pairs sorted in a decreasing order of relative frequency. 
3. Repeat the above steps using at least 2 VM instances in Hadoop system running in a fully-distributed mode.
